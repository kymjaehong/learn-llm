{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 구성\n",
    "\n",
    "### 1) 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain langchain-openai tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) OpenAI 인증키 설정\n",
    "\n",
    "https://openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Chain\n",
    "\n",
    "### 1) Prompt + LLM\n",
    "- 가장 기본적이고 일반적인 사용 사례\n",
    "- 프롬프트 템플릿과 모델을 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# model\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125')\n",
    "\n",
    "# chain 실행\n",
    "resp = llm.invoke('지구의 자전 주기는?')  # AIMessage object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# promt + model + output parser\n",
    "prompt = ChatPromptTemplate.from_template(\"You are an expert in astronomy, Answer the question, <Question>: {input}\")\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125')\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# LCEL chaining\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# chain 호출\n",
    "chain.invoke({\"input\": \"지구의 자전 주기는?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"translates {korean_word} to English\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"explain {english_word} using oxford dictionary to me in Korean.\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo-0125')\n",
    "\n",
    "chain1 = prompt1 | llm | StrOutputParser()\n",
    "\n",
    "chain1.invoke({\"korean_word\": \"미래\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = (\n",
    "    {\"english_word\": chain1}\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()    \n",
    ")\n",
    "\n",
    "chain2.invoke({\"korean_word\": \"미래\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prompt\n",
    "- 프롬프트: 사용자가 언어 모델에 입력하는 질문이나 요청\n",
    "- 모델이 제공할 응답의 방향과 내용을 결정하는 데 중요한 역할\n",
    "- 프롬프트 작성 원칙\n",
    "    1. 명확성과 구체성\n",
    "        - 질문은 명확하고 구체적이어야 합니다. 모호한 질문은 LLM 모델의 혼란을 초래할 수 있기 때문입니다.\n",
    "        - 예시: \"다음 주 주식 시장에 영향을 줄 수 있는 예정된 이벤트들은 무엇일까요?\"는 \"주식 시장에 대해 알려주세요.\"보다 더 구체적이고 명확한 질문입니다.\n",
    "    2. 배경 정보를 포함\n",
    "        - 모델이 문맥을 이해할 수 있도록 필요한 배경 정보를 제공하는 것이 좋습니다. 이는 환각 현상(hallucination)이 발생할 위험을 낮추고, 관련성 높은 응답을 생성하는 데 도움을 줍니다.\n",
    "        - 예시: \"2020년 미국 대선의 결과를 바탕으로 현재 정치 상황에 대한 분석을 해주세요.\"\n",
    "    3. 간결함\n",
    "        - 핵심 정보에 초점을 맞추고, 불필요한 정보는 배제합니다. 프롬프트가 길어지면 모델이 덜 중요한 부분에 집중하거나 상당한 영향을 받는 문제가 발생할 수 있습니다.\n",
    "        - 예시: \"2021년에 발표된 삼성전자의 ESG 보고서를 요약해주세요.\"\n",
    "    4. 열린 질문 사용\n",
    "        - 열린 질문을 통해 모델이 자세하고 풍부한 답변을 제공하도록 유도합니다. 단순한 '예' 또는 '아니오'로 대답할 수 있는 질문보다는 더 많은 정보를 제공하는 질문이 좋습니다.\n",
    "        - 예시: \"신재생에너지에 대한 최신 연구 동향은 무엇인가요?\"\n",
    "    5. 명확한 목표 설정\n",
    "        - 얻고자 하는 정보나 결과의 유형을 정확하게 정의합니다. 이는 모델이 명확한 지침에 따라 응답을 생성하도록 돕습니다.\n",
    "        - 예시: \"AI 윤리에 대한 문제점과 해결 방안을 요약하여 설명해주세요.\"\n",
    "    6. 언어와 문체\n",
    "        - 대화의 맥락에 적합한 언어와 문체를 선택합니다. 이는 모델이 상황에 맞는 표현을 선택하는데 도움이 됩니다.\n",
    "        - 예시: 공식적인 보고서를 요청하는 경우, \"XX 보고서에 대한 전문적인 요약을 부탁드립니다.\"와 같이 정중한 문체를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) PromptTemplate\n",
    "- PromptTemplate + LLMs (단일 문장 입력 -> 단일 문장 출력)\n",
    "- **문자열 프롬프트**를 위한 템플릿을 생성. Python의 문자열 포맷팅 구문을 사용.\n",
    "- 내용: 지시사항, 몇 가지 예시, 특정 맥락 및 질문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ChatPromptTemplate\n",
    "- chatPromptTemplates + ChatModels (여러 메시지 입력 -> 단일 메시지 출력)\n",
    "- 채팅 메시지를 원소로 갖는 리스트 형태\n",
    "- 구성: 각 채팅 메시지는 역할(role)과 내용(content)이 짝을 이루는 형태\n",
    "    - 예시: OpenAI는 AI Assistant, Human, System 등의 역할(role)로 구성\n",
    "\n",
    "1. Message 유형\n",
    "- SystemMessage: 시스템의 기능을 설명합니다.\n",
    "- HumanMessage: 사용자의 질문을 나타냅니다.\n",
    "- AIMessage: AI 모델의 응답을 제공합니다.\n",
    "- FunctionMessage: 특정 함수 호출의 결과를 나타냅니다.\n",
    "- ToolMessage: 도구 호출 결과를 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-튜플 형태의 메시지 목록으로 프롬프트 생성 (type, content)\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    {\"system\": \"이 시스템은 천문학 질문에 답변할 수 있습니다.\"},\n",
    "    {\"user\": \"{user_input}\"},\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇일까요?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-MessagePromptTemplate 활용\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇일까요?\")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke({\"user_input\": \"태양계에서 가장 큰 행성은 무엇일까요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Parameter\n",
    "\n",
    "### 1) 파라미터 설정\n",
    "- Temperature: 값이 작으면 예측 가능하고 일관된 출력을 생성하는 반면, 값이 크면 다양하고 예측하기 어려운 출력을 생성합니다.\n",
    "- Max Tokens(최대 토큰 수): 생성할 텍스트의 길이를 제한합니다.\n",
    "- Top P(Probability): 특정 확률 분포 내에서 상위 P% 토큰만을 고려하는 방식입니다.\n",
    "- Frequency Penalty: 값이 클수록 이미 등장한 단어나 구절이 다시 등장할 확률을 감소시킨다.\n",
    "- Presence Penalty: 값이 클수록 아직 텍스트에 등장하지 않은 새로운 단어의 사용이 장려됩니다. \n",
    "- Stop Sequences: 출력을 특정 포인트에서 종료하고자 할 때 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 파라미터 설정\n",
    "params = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "\n",
    "kwargs = {\n",
    "    \"frequency_penalty\": 0.5,\n",
    "    \"presence_penalty\": 0.5,\n",
    "    \"stop\": [\"\\n\"],\n",
    "}\n",
    "\n",
    "# 모델 인스턴스를 생성할 때 설정\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo-0125', **params, model_kwargs=kwargs)\n",
    "\n",
    "# 모델 호출\n",
    "q = \"태양계에서 가장 큰 행성은 무엇인가요?\"\n",
    "resp = model.invoke(q)\n",
    "\n",
    "resp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 인스턴스를 호출할 때 전달\n",
    "params = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 100,\n",
    "}\n",
    "\n",
    "resp = model.invoke(input=q, **params)\n",
    "\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델에 추가적인 파라미터 전달\n",
    "- bind 메서드를 사용하여 체인에 새로운 파라미터를 추가하여 연결 가능\n",
    "- 다양한 상황에 맞게 모델의 동작을 제어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"이 시스템은 천문학 질문에 답변할 수 있습니다.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(model='gpt-3.5-turbo-0125', max_tokens=100)\n",
    "\n",
    "messages = prompt.format_messages(user_input=\"태양계에서 가장 큰 행성은 무엇일까요?\")\n",
    "\n",
    "before_answer = model.invoke(messages)\n",
    "\n",
    "print(before_answer)\n",
    "\n",
    "# 모델 호출 시, 추가적인 인수를 전달하기 위해 bind 메서드 사용\n",
    "chain = prompt | model.bind(max_tokens=10)\n",
    "\n",
    "after_answer = chain.invoke(messages)\n",
    "\n",
    "print(after_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
